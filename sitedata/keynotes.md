We are excited to share the following speakers have kindly accepted to give invited talks at INLG2024.

## Keynote 1: [Yulan He](https://sites.google.com/view/yulanhe)

**Time:** 25th Sep. 13:30-14:15

**Title:** Enhancing LLM Reasoning through Reflection and Refinement

**Abstract:** The reasoning capabilities of Large Language Models (LLMs) can be greatly improved through reflection and refinement techniques. This talk delves into several scenarios illustrating these advancements. First, we explore how LLM self-refinement, guided by multi-perspective solution seeking and self-consistency checks, produces superior results in question answering (QA) compared to traditional self-reflection approaches. Second, in the domain of student answer scoring, we decompose the scoring process into a series of binary questions that evaluate the presence of key answer elements. This method constructs a thought tree, enabling the generation of rationales and facilitating explainable scoring. Third, we address the challenge of causal event extraction. Unlike many current findings, LLM evaluators do not align well with human evaluation results. To address this,, we propose training a smaller language model on human evaluation data to serve as an evaluator and reward model, allowing us to develop a robust causal event extraction system. Lastly, in the context of mystery murder games, we demonstrate that equipping LLMs with multiple sensory inputs allows for better situational evaluation and more effective navigation in the search for suspects. The talk will conclude with an exploration of future research directions for further enhancing the reasoning capabilities of LLMs.

**Short Bio:** Yulan He is a Professor in Natural Language Processing at the Department of Informatics in King’s College London, UK. She directs [the NLP group there](https://kclnlp.github.io). Yulan obtained her PhD degree from the University of Cambridge. She is currently holding a prestigious 5-year UKRI Turing AI Fellowship. Yulan’s research interests lie in the integration of machine learning and natural language processing for text understanding. Recently, she has focused on addressing the limitations of Large Language Models (LLMs), aiming to enhance their reasoning capabilities, robustness, and explainability. She has published over 250 papers on topics such as machine reading comprehension, model interpretability and trustworthy AI, NLP for health, finance and education. She has received several prizes and awards for her research, including a SWSA Ten-Year Award, a CIKM Test-of-Time Award, and AI 2020 Most Influential Scholar Honourable Mention. She served as the General Chair for AACL-IJCNLP 2022 and a Program Co-Chair for various conferences such as ECIR 2024, CCL 2024, and EMNLP 2020. Her research has received support from the EPSRC, Royal Academy of Engineering, EU-H2020, Innovate UK, British Council, and industrial funding.


## Keynote 2: [Mark Riedl](https://eilab.gatech.edu/mark-riedl.html)

**Time:** 26th Sep. 10:00-10:45

**Title:** TBA

## Keynote 3: [Kees van Deemter](https://www.uu.nl/medewerkers/CJvanDeemter)

**Time:** 26th Sep. 13:30-14:15

**Title:** TBA

**Abstract:** Large Language Models (LLMs) have hauled Natural Language Generation (NLG) away from the fringes of NLP, into the spotlight. These developments are offering the NLG community considerable exposure and enormous opportunities. But with great renown come great responsibilities. Foremost among these responsibilities is the ability to assess the quality of generated texts reliably. In this talk, parts of which rest on joint work with Eduardo Calo, Albert Gatt, and Saad Mahamood, I will argue that we have not yet fulfilled that responsibility because, as yet, there exists no "theory" that tells us what are the different kinds of flaws from which a generated text can suffer, and how each of these flaws should be weighed.

To illustrate these points, I will zoom in on a family of flaws that have attracted considerable attention and that should be relatively straightforward to understand. They are associated with the word "hallucination". Focussing primarily on Data-text NLG, I will argue that recently proposed perspectives on hallucination are not as systematic, clear, and fine-grained as they should be. Having done this, I will explore an alternative, logic-based perspective and discuss the strengths and limitations of this perspective in light of some experiments with an annotation scheme derived from this alternative perspective, which were recently conducted at trivago in Dusseldorf. I will conclude by asking what still separates the NLG community from possessing a systematic, clear, and fine-grained understanding of, firstly, hallucination and, secondly, other flaws from which a generated text can suffer, many of which can be as harmful as hallucination but much harder to grasp.

<span style="font-size: 0.8rem; color: #666666">K. van Deemter (2024) The Pitfalls of Defining Hallucination. Computational Linguistics, June 2024.</span>

**Short Bio:** Kees van Deemter has worked in Natural Language Processing from 1984 and on Natural Language Generation from about 1994. He likes to work with scholars in neighbouring disciplines, such as linguists, logicians, and psycholinguists. He is the author of "Not Exactly: in Praise of Vagueness" (Oxford University Press 2010) and of "Computational Models of Referring: a Study in Cognitive Science" (MIT Press 2016). Having led Utrecht University's NLP group from 2018 until March 2024, he is currently an Emeritus Professor at Utrecht University.

## Keynote 4: [Koichiro Yoshino](https://www.pomdp.net/)

**Time:** 27th Sep. 13:30-14:15

**Title:** Embodied Language Generation for Autonomous Robot

**Abstract:** With the improved performance provided by large language models, language generation systems are being used in a wide variety of applications, including robotics. In the field of robotics, language generation systems are used not only for response generation but also for robot's action planning. Embodiment is essential for utilizing language models in such robotic tasks. When performing language generation from the vast amount of real-world information available to the robot, it is important to choose which information to use and to set up a point of view from which to make that choice. This talk will focus on these points while introducing our efforts related to embodied language generation systems for robots.

**Short Bio:** Koichiro Yoshino is an Associate Professor at Tokyo Institute of Technology, a Team Leader at the Institute of Physical and Chemical Research (RIKEN), and an Affiliate Professor of Nara Institute of Science and Technology (NAIST). He received his bachelor's degree of arts from Keio University in 2009, master's degree in informatics from Kyoto University in 2011, and Ph.D. in informatics from Kyoto University in 2014. He worked at Kyoto University as a postdoc and NAIST as an assistant professor. From 2024, a cross-appointment between an Associate Professor at School of Computing, Tokyo Institute of Technology and a Team Leader at Guardian Robot Project, RIKEN. From 2019 to 2020, he was a visiting researcher of Heinrich-Heine-Universität Düsseldorf, Germany. He is working on areas of spoken and natural language processing, especially robot dialogue systems. Dr. Koichiro Yoshino received several honors, including the best paper award of IWSDS2020, IWSDS2024, and the best paper award of the 1st NLP4ConvAI workshop. He is a member of IEEE-SLTC, a member of DSTC Steering Committee, an action editor of ARR, a board member of SIGdial and a board member of ANLP. He is a senior member of IPSJ and a member of JSAI and RSJ.