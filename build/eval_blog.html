<!DOCTYPE html>
<html lang="en">

<head>

  <!-- Required meta tags -->
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />

  <!-- External Javascript libs_ext  -->
  <script src="https://cdn.jsdelivr.net/npm/d3@5/dist/d3.min.js"></script>

  <script src="https://cdn.jsdelivr.net/npm/handlebars@4.7.3/dist/handlebars.min.js"
    integrity="sha256-/PJBs6QWvXijOFIX04kZpLb6ZtSQckdOIavLWKKOgXU=" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"
    integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
    integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
    crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.4.1/dist/js/bootstrap.min.js"
    integrity="sha256-WqU1JavFxSAMcLP2WIOI+GB2zWmShMI82mTpLDcqFUg=" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/moment@2.24.0/min/moment.min.js"
    integrity="sha256-4iQZ6BVL4qNKlQ27TExEhBN1HFPvAvAMbFavKKosSWQ=" crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/moment-timezone@0.5.28/builds/moment-timezone-with-data.min.js"
    integrity="sha256-IWYg4uIC8/erItNXYvLtyYHioRi2zT1TFva8qaAU/ww=" crossorigin="anonymous"></script>


  <!-- Library libs_ext -->
  <script src="static/js/libs_ext/typeahead.bundle.js"></script>


  <!--    Internal Libs -->
  <script src="static/js/data/api.js"></script>


  <script>
    var auth0_domain = "dev-ma80app1.us.auth0.com";
    var auth0_client_id = "ptF3gg7K6X0VN0FxOeCAVbKURrAis82d";
  </script>
  <script src="https://cdn.auth0.com/js/auth0-spa-js/1.7/auth0-spa-js.production.js"></script>
  <script src="static/js/modules/auth0protect.js"></script>


  <!-- External CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.3.1/dist/css/bootstrap.min.css"
    integrity="sha256-YLGeXaapI0/5IgZopewRJcFXomhRMlYYjugPLSyNjTY=" crossorigin="anonymous">

  <!-- External Fonts (no google for china) -->
  <link href="static/css/Lato.css" rel="stylesheet" />
  <link href="static/css/Exo.css" rel="stylesheet" />
  <link href="static/css/Cuprum.css" rel="stylesheet" />
  <link rel="stylesheet" href="static/css/main.css" />
  <!--    <link rel="stylesheet" href="static/css/fa_regular.css"/>-->
  <link rel="stylesheet" href="static/css/fa_solid.css" />
  <link rel="stylesheet" href="static/css/lazy_load.css" />
  <link rel="stylesheet" href="static/css/typeahead.css" />

  <title>INLG 2024</title>

</head>

<body>
  <!-- NAV -->

  <nav class="navbar sticky-top navbar-expand-lg navbar-light bg-light mr-auto" id="main-nav">
    <div class="container">
      <a class="navbar-brand" href="index.html">
        <img class="logo" src="static/images/inlg_logo.png" height="auto" width="90px" />
      </a>

      <a class="navbar-brand" href="index.html">17th International Natural Language Generation Conference</a>

      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse text-right flex-grow-1" id="navbarNav">
        <ul class="navbar-nav ml-auto">
          <li class="nav-item active">
            <a class="nav-link" href="index.html">Home</a>
          </li>

          <li class="nav-item ">
            <a class="nav-link" href="calls.html">Calls</a>
          </li>

          <li class="nav-item ">
            <a class="nav-link" href="workshops.html">Workshops</a>
          </li>

          <li class="nav-item ">
            <a class="nav-link" href="organizers.html">Organizers</a>
          </li>

          <li class="nav-item ">
            <a class="nav-link" href="https://sigdialinlg2024.github.io/">Participating</a>
          </li>

          <li class="nav-item dropdown">
            <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown"
              aria-haspopup="true" aria-expanded="false">
              Program
            </a>
            <div class="dropdown-menu" aria-labelledby="navbarDropdown">
              <a class="dropdown-item" href="speakers.html">Keynotes</a>
              <a class="dropdown-item" href="https://sigdialinlg2024.github.io/calendar.html">Schedule</a>
              <!--                    <a class="dropdown-item" href="panels.html">Panel</a>-->
              <a class="dropdown-item" href="accepted_papers.html">Accepted Papers</a>
              <a class="dropdown-item" href="awards.html">Awards</a>
              <!--                    <a class="dropdown-item" href="papers.html">Papers</a>-->
              <!--                    <a class="dropdown-item" href="workshops.html">Workshops</a>-->
              <!--                    <a class="dropdown-item" href="tutorials.html">Tutorials</a>-->
              <!--                    <a class="dropdown-item" href="hackathons.html">Hackathon</a>-->
              <!--                    <a class="dropdown-item" href="genchal.html">GenChal</a>-->
            </div>
          </li>

          <!--               <li class="nav-item ">-->
          <!--                  <a class="nav-link" href="calendar.html">Schedule</a>-->
          <!--              </li>-->

          <!--                <li class="nav-item ">-->
          <!--                    <a class="nav-link" href="sponsors.html">Sponsors</a>-->
          <!--                </li>-->

          <!--                <li class="nav-item ">-->
          <!--                    <a class="nav-link" href="registration.html">Registration</a>-->
          <!--                </li>-->

          <li class="nav-item ">
            <a class="nav-link" href="help.html">Help</a>
          </li>
          <li class="nav-item ">
            <a class="nav-link" href="https://2024.sigdial.org/">SIGDIAL 2024</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>



  <!-- User Overrides -->

  <style>
    .back {
      background: url("static/images/tokyo.jpg") no-repeat center;
      background-size: cover;
    }

    .back h2 {
      font-family: "Lato", sans-serif;
      font-weight: 900;
    }
  </style>

  <div class="jumbotron jumbotron-fluid back" style="padding-bottom: 20px;">
    <div class="row header p-6 m-5">
      <div class="p-1 mx-auto text-center text-white col-md-9 col-sm-12">
        <h2>
          17th International Natural Language Generation Conference
        </h2>
        <h2 class="p-5">
          September 2024
        </h2>
      </div>
    </div>
  </div>


  <div class="container">
    <!-- Tabs -->
    <div class="tabs">

    </div>
    <!-- Content -->
    <div class="content">


      <div class="container-fluid">
        <div class="row">
          <div class="col-sm border">
            <p><strong>Evaluating NLG systems: a brief introduction</strong></p>
            <p><em>Emiel van Miltenburg</em></p>
            <p>This year INLG will feature an award for the paper with the best evaluation. The purpose of this award is
              to provide an incentive for NLG researchers to pay more attention to the way they assess the output of
              their systems. This blog provides a short introduction to evaluation in NLG, explaining key terms and
              distinctions.</p>
            <p><strong>How can I evaluate my system?</strong></p>
            <p>It is hard to say in general how you should evaluate your NLG system. Much depends on the kind of system
              that you are developing, and the context in which it is being used. A first step is to get used to
              commonly used terminology in the field, so that you know what possibilities are out there.</p>
            <p><em>Intrinsic evaluation</em> refers to the assessment of system output in isolation. For example,
              grammaticality is a property that you can assess using intrinsic evaluation. You could use either
              <em>human evaluation</em> (e.g. grammaticality judgments) or <em>automatic metrics</em> (e.g. a precision
              grammar or a grammar checker) to determine whether the output of an NLG system is grammatical. (For an
              overview of different properties that you could evaluate, see <a
                href="https://aclanthology.org/2020.inlg-1.24/">Belz et al. , 2020</a>.) Finally, you could carry out an
              <em>error analysis</em> to determine where the system still falls short.</p>
            <p><em>Extrinsic evaluation</em> refers to the impact that a system may have on downstream processes. If you
              have a newly developed NLG system, you could for example see whether employees become more efficient or
              more productive after the system has been deployed.</p>
            <p>Sometimes people also use the term “extrinsic evaluation” to assess the impact of a specific module in an
              NLG pipeline. An <em>intrinsic</em> evaluation of an NLG pipeline module would just look at the quality of
              its output, rather than the final text that is produced by the full pipeline. You could carry out an
              <em>extrinsic</em> evaluation by determining the extent to which the final output improves when you
              replace an existing module (e.g. for rule-based referring expression generation) with a newly developed
              module. </p>
            <p>If you are developing multiple new modules, you could carry out <em>ablation tests</em> (systematically
              leaving out or replacing different modules) to see how much each new module contributes to the system’s
              overall performance.</p>
            <p>For a more in-depth overview, see e.g. <a href="https://arxiv.org/abs/2006.14799">Celikyilmaz et al.
                (2020)</a> or <a href="https://dl.acm.org/doi/abs/10.1145/3485766">Sai et al. (2022)</a>.</p>
            <p><strong>Human versus automatic evaluation</strong></p>
            <p>Human evaluation is generally seen as the gold standard in NLG research, because in the end it is
              essential that human readers appreciate the output of your system. Having that said, there is great value
              in reliable automatic metrics, since they are cheaper and not as labor-intensive. Reliability is the
              keyword here: no matter how you measure different properties of the generated text, we have to be able to
              trust the conclusions that you draw from your observations.</p>
            <p><strong>What are current best practices in the field?</strong></p>
            <p>It is always risky to talk about best practices, because evaluation is so context-dependent. As the
              saying goes: “a foolish consistency is the hobgoblin of little minds” (Emerson 1841). For some projects,
              it may be better to deviate from existing standards. Having said that, here are ten steps you may find
              useful in planning your evaluation. Some of these steps raise fundamental questions about your research
              project, so it is important to start thinking about evaluation at the start of your project, and to not
              consider evaluation as an afterthought. Moreover, good evaluations take time, which means you need to
              schedule enough time to carry out a reliable evaluation.</p>
            <ol>
              <li>
                <p><em>Determine the target audience.</em> Who are you developing your NLG system for? Ideally you will
                  evaluate the performance of your system with a group of participants that matches the demographic
                  properties of your target audience. Even if you don’t carry out a human evaluation, it is still
                  important to understand the application context because of the next point.</p>
              </li>
              <li>
                <p><em>Get to know user needs</em>. If you have an applied NLG project, you should ideally start from an
                  understanding of the stakeholders’ needs. That means talking to the people who will use your system,
                  getting to know what they want to do with your system and what properties are important for them. Then
                  you can develop an evaluation protocol that is in line with user needs.</p>
              </li>
              <li>
                <p><em>Identify relevant work.</em> Search for relevant literature, identifying models you want to
                  compare to and metrics that are commonly used. Determine if those metrics make sense given your
                  project. Consider reproducing relevant results, to be able to carry out any comparisons yourself.</p>
              </li>
              <li>
                <p><em>Determine your goals and expectations</em>. Based on the earlier steps, formulate a relevant
                  research question. Think about possible outcomes of your project, which outcome is more likely, and
                  why you expect this to be the case. Also consider how different outcomes of your evaluation should be
                  interpreted. Try to be as precise as possible. Does it make sense to form hypotheses about your
                  experiments, and to motivate them on the basis of earlier literature? This helps with theory building
                  in NLG. </p>
              </li>
              <li>
                <ul>
                  <li><em>Identify key independent variables.</em> There are many factors that could influence the
                    characteristics of the output text. Different sets of inputs lead to different kinds of outputs, and
                    different system properties affect the way the output text looks. Your job is to identify the main
                    variables of interest.</li>
                </ul>
              </li>
              <li>
                <p><em>Determine key dependent variables</em>. There are many different properties of NLG output that
                  you could assess, for example: grammaticality, fluency, completeness, naturalness, appropriateness,
                  and the list goes on. You don’t have space to cover them all (although you could provide an additional
                  extensive evaluation in the appendix), and some properties are probably more relevant than others for
                  the purpose of your project. Clearly define the constructs of interest before you start thinking about
                  how to operationalise those constructs. Use those definitions to critically assess the metrics you are
                  planning to use.</p>
              </li>
              <li>
                <p><em>Check the validity of your set-up.</em> Having identified all relevant variables, you can
                  operationalise the different dependent variable through different kinds of metrics. Here it is
                  important to ensure the validity of your metrics. In other words: do your metrics measure what they
                  are supposed to measure? One way to ensure the validity of a metric is to study the correlation
                  between that metric and a trusted reference, such as human judgments. (Ehud Reiter <a
                    href="https://aclanthology.org/J18-3002/">investigated</a> this in detail for the BLEU metric.)
                  Alternatively you could think of theoretical arguments to motivate why your metric provides a good
                  approximation of the variable of interest.</p>
              </li>
              <li>
                <p><em>Select a sensible subset for evaluation.</em> If you cannot evaluate all the output of your model
                  (and possibly the models that you are comparing your work to), think about the way you are sampling
                  the outputs-to-be-evaluated. The sampling procedure heavily impacts the validity of your evaluation
                  and the generalisability of your results.</p>
              </li>
              <li>
                <p><em>Get IRB approval (if appropriate for your study).</em> When you know what the evaluation will
                  look like, you can apply for approval with your local institutional review board (IRB, also known as
                  ‘ethics committee’) to determine that your study follows current ethics guidelines. If you do this at
                  the onset of your project, you are less likely to run into any procedural delays. With a research
                  proposal for the IRB in hand, you may also decide to turn the proposal into a full preregistration of
                  your study (see <a href="https://aclanthology.org/2021.naacl-main.51/">Van Miltenburg et al.,
                    2021</a>).</p>
              </li>
              <li>
                <p><em>Keep a log.</em> Carry out your study and note any deviations from your original plans, including
                  the reasons why you changed your mind. These insights are essential to provide the rationale behind
                  your study design. If you do not write this information down, you will forget it. For example, you
                  could create a private GitHub repository to hold all your code, data, and notes. (Once your project is
                  done, you can publish the repository alongside your paper. There are also <a
                    href="https://anonymous.4open.science">services</a> to create a link to an anonymised version of
                  your repository, that you can include in your submitted paper.) Or you could manage your project
                  through the <a href="https://osf.io">Open Science Foundation</a> (OSF). </p>
              </li>
              <li>
                <p><em>Be explicit about your materials and methods.</em> Report all relevant information about how you
                  carried out your evaluation, so that others could reproduce your work using only your paper. If you
                  carry out a human evaluation, the <a href="https://aclanthology.org/2022.humeval-1.6/">Human
                    Evaluation DataSheet</a> provides an overview of important details to record.</p>
              </li>
              <li>
                <p><em>Describe all relevant results</em>. If you are reporting overall scores, consider providing a
                  table with disaggregated results for different subsets of the input data. Next to overview tables, you
                  may also want to create insightful visualisations of the results. (Though choose wisely; don’t just
                  duplicate your results in another modality.) Go beyond the “higher is better” narrative and explain
                  what the results mean for your system and the NLG literature in general. Be open about the limitations
                  of your evaluation and the challenges that still lie ahead. </p>
              </li>
            </ol>
            <p><strong>Bonus tip</strong>: <em>archive all data associated with your study.</em> This includes including
              all outputs for the validation and test sets, crowdsourcing templates, aggregated and <a
                href="https://pdai.info">non-aggregated</a> human ratings, outputs of statistical analysis software).
              Small files might be included in your GitHub repository, but otherwise data can be hosted through other
              services, e.g. OSF, <a href="https://zenodo.org">Zenodo</a>, <a href="https://figshare.com">Figshare</a>,
              organisational repository or national science hosting provider. It is possible that not all data can be
              made available at submission time (though it is often possible to share data anonymously), but at least
              try to be as exhaustive as possible for your camera-ready version.</p>
            <p>For more in-depth reading, here are some useful references:</p>
            <ul>
              <li><a href="https://aclanthology.org/2021.inlg-1.14/">Van Miltenburg et al. (2021)</a> provide guidelines
                for error analysis.</li>
              <li><a href="https://www.sciencedirect.com/science/article/pii/S088523082030084X">Van der Lee et al.
                  (2021)</a> provide recommendations for human evaluation studies.</li>
              <li><a href="https://arxiv.org/abs/2202.06935">Gehrmann et al. (2022)</a> provide general recommendations
                regarding both human and automatic evaluations.</li>
              <li><a href="https://ehudreiter.com/blog-index/">Ehud Reiter’s blog</a> has more recommendations for
                evaluating NLG systems.</li>
            </ul>
            <p>If you have any questions, the community discord channel is a great place to ask questions. Contact Dave
              Howcroft (<a href="mailto:D.Howcroft@napier.ac.uk">D.Howcroft@napier.ac.uk</a>) for an invitation.</p>
            <p><em>Thanks to Simone Balloccu, Ondřej Dušek, Dave Howcroft, Maria Keet, Ember Manning, Maja Popović,
                Craig Thomson, and Sina Zarrieß for feedback on an earlier draft of this blog post.</em></p>
          </div>
          <!--    <div class="col col-lg-2">-->
          <!--      <a class="twitter-timeline" data-height="500" href="https://twitter.com/inlgmeeting?ref_src=twsrc%5Etfw">Tweets by inlgmeeting</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>-->
          <!--    </div>-->
        </div>
      </div>


      <script>
        const updateUI = async () => {
          const auth0_domain = "dev-ma80app1.us.auth0.com";
          const auth0_client_id = "ptF3gg7K6X0VN0FxOeCAVbKURrAis82d";
          const auth0 = await createAuth0Client({
            domain: auth0_domain,
            client_id: auth0_client_id,
            cacheLocation: "localstorage",
          });
          const is_auth = await auth0.isAuthenticated();
          console.log(is_auth, "--- is_auth");
          if (is_auth) {
            document.body.style.display = null;

            // unused atm, hook up later; this won't get executed since we change location above
            const user = await auth0.getUser();
            $(".loginBtn").hide();
            $(".logoutBtn").show();
            $(".secret").show();
            $(".user_name").text(user.name);
            $(".login-message").text("You are logged in as");
          } else {
            $(".loginBtn").show();
            $(".logoutBtn").hide();
            $(".secret").hide();
            $(".user_name").text("");
            $(".login-message").text("You are currently not authenticated.");
          }

          $(".loginBtn").click(async function () {
            window.location.href = `redirect.html?return=index.html`;
          });
          $(".logoutBtn").click(async function () {
            await auth0.logout({
              returnTo: window.location.href,
            });
          });
        };
        updateUI();
      </script>



    </div>
  </div>



  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag("js", new Date());
    gtag("config", "UA-");
  </script>

  <!-- Footer -->
  <footer class="footer bg-light p-4">
    <div class="container">
      <p class="float-right"><a href="#">Back to Top</a></p>
      <p class="text-center">© 2020 INLG 2024 Organizers</p>
    </div>
  </footer>

  <!-- Code for hash tags -->
  <script type="text/javascript">
    $(document).ready(function () {
      if (window.location.hash !== "") {
        $(`a[href="${window.location.hash}"]`).tab("show");
      }

      $("a[data-toggle='tab']").on("shown.bs.tab", function (e) {
        const hash = $(e.target).attr("href");
        if (hash.substr(0, 1) === "#") {
          const position = $(window).scrollTop();
          window.location.replace(`#${hash.substr(1)}`);
          $(window).scrollTop(position);
        }
      });
    });
  </script>
  <!--    <script src="static/js/modules/lazyLoad.js"></script>-->

</body>

</html>